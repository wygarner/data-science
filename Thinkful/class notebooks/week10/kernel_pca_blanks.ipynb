{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "np.random.seed(42)\n",
    "\n",
    "a1 = np.random.normal(-10, 1, n // 3)\n",
    "a2 = np.random.normal(10, 1, n // 3)\n",
    "b = np.random.normal(0, 1, n // 3)\n",
    "\n",
    "x = np.hstack((a1, b, a2))\n",
    "\n",
    "labels = [\"a\"] * (n // 3)\n",
    "labels += [\"b\"] * (n // 3)\n",
    "labels += [\"a\"] * (n // 3)\n",
    "\n",
    "df = pd.DataFrame({\"x\": x, \"y\": 0, \"label\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot `x` by `y` and color by `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'>\n",
    "  <img src='https://i.imgur.com/xcRD0xC.png' width=75%>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a homemade 'kernel' to map our data to a higher dimension.\n",
    "\n",
    "* How are we able to tell how to separate the classes?\n",
    "* How can we make the numbers reflect what we're seeing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kernel_y\"] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the data using `kernel_y` instead of `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom, kerneled\n",
    "\n",
    "This worked here, but it'd be nice if there was a preset selection of kernels that work in a lot of cases.................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA to the `X`.  Only ask for a single principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\", \"kernel_y\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the data, this time use:\n",
    "* the 1st principal component as your x axis data\n",
    "* 0 as the y axis data\n",
    "* color by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unrealistic example to show the concept of what `KernelPCA` (and what kernels in SVM are doing).  The overall process:\n",
    "\n",
    "* We map our data into a higher dimension using a kernel (aka data is mapped to kernel space)\n",
    "* We then apply our analysis on this higher dimensional data (in `KernelPCA` we would apply PCA; in Kernel SVM we would apply a linear SVM)\n",
    "\n",
    "In a more realistic example, we would likely have more than one feature to start and the application of the kernel would be less straightfoward then squaring the feature.  In practice we'll use one of the predefined kernels that `sklearn` provides.\n",
    "\n",
    "Let's see how `sklearn.decomposition.KernelPCA` would treat the same problem with different kernels/kernel parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `KernelPCA` to the `X` and replot.  Try different parameters for the kernel. How is it doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another toy data example to show `KernelPCA` succeeding before looking at real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = [\"x\", \"y\"]\n",
    "df[\"label\"] = y\n",
    "\n",
    "sns.scatterplot(\"x\", \"y\", hue=\"label\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply `PCA` to the `X` to reduce to 1 dimension\n",
    "* Plot the resulting first principal component and color by `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply `KernelPCA` to the `X` to reduce to 1 dimension\n",
    "* Plot the resulting first principal component and color by `y`\n",
    "* Play with parameters. Are we able to make the data linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're wanting to use `KernelPCA` in a supervised learning pipeline.  I suggest to use `sklearn.pipeline.Pipeline` and optimize these paramaters with `sklearn.model_selection.GridSearchCV` (or a different search like `BayesSearchCV` to speed things up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to some boring real data to see how a pipeline might look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Restricting to just sepal data\n",
    "X = iris[[\"sepal_length\", \"sepal_width\"]]\n",
    "y = iris[\"species\"]\n",
    "\n",
    "sns.scatterplot(\"sepal_length\", \"sepal_width\", hue=\"species\", data=iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this might look manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "\n",
    "kpca = KernelPCA(2, kernel=\"rbf\", gamma=10)\n",
    "pcs = kpca.fit_transform(X)\n",
    "\n",
    "pc_df = pd.DataFrame({\"pc1\": pcs[:, 0], \"pc2\": pcs[:, 1]})\n",
    "pc_df[\"label\"] = iris[\"species\"]\n",
    "\n",
    "sns.scatterplot(\"pc1\", \"pc2\", hue=\"label\", data=pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pc_model = LogisticRegression()\n",
    "no_pc_model.fit(scaled, y)\n",
    "no_pc_acc = no_pc_model.score(scaled, y)\n",
    "\n",
    "pc_model = LogisticRegression()\n",
    "pc_model.fit(pcs, y)\n",
    "pc_acc = pc_model.score(pcs, y)\n",
    "\n",
    "print(f\"No KernalPCA Accuracy: {no_pc_acc}\")\n",
    "print(f\"KernalPCA Accuracy: {pc_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well those parameters didn't work too well..  Lets try and optimize\n",
    "\n",
    "Make a `Pipeline` and grid of parameters to optimize `KernelPCA`.  Run a `GridSearchCV` to find the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gonna get (and ignore) ConvergenceWarnings\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "#       ('step_name', Step()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {}\n",
    "\n",
    "pipeline_cv = GridSearchCV(pipeline, params)\n",
    "pipeline_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cv.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That accuracy looks familiar.... lets look at the selected parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up choosing a linear kernel with 2 components.  This means that our KernelPCA step didn't add anything to our pipeline (we just rotated our data and passed it to the classification step).\n",
    "\n",
    "Like any method we've seen, this isn't a silver bullet.  Try some things and find out what works, and use a grid search to help you out along the way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
