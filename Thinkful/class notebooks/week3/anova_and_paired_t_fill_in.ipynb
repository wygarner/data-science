{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "A study was done to see if different forms of audio would have an effect on the level of pain reported by patients.  The data is made up, but in a way to draw the same conclusions.  Original study can be found [here](https://www.npr.org/sections/health-shots/2015/06/22/415048075/to-ease-pain-reach-for-your-playlist-instead-of-popping-a-pill).\n",
    "\n",
    "<p align='center'>\n",
    "    <img width='50%' src='https://whyy.org/wp-content/uploads/2017/07/painscale-768x432.jpg'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fmt: off\n",
    "audio_book = np.array([7.5, 4. , 4. , 3. , 6.5, 1. , 8. , 4. , 5.5, 4.5, 7.5, 1.5, 4.5,\n",
    "                       4.5, 7. , 3. , 4.5, 3.5, 5. , 6. , 3. , 7. , 6.5, 6. , 6.5, 4. ,\n",
    "                       5. , 3.5, 4.5, 6. , 4. , 4.5, 4. , 3.5, 4. , 5. , 3. , 5.5, 8. ,\n",
    "                       6.5, 4.5, 3.5, 4. , 8. , 5. , 4. , 5.5, 8.5, 5. , 6. , 5.5, 4.5,\n",
    "                       3. , 4.5, 4.5, 6. , 6.5, 6.5, 5.5, 6.5, 4. , 7. , 6. , 4.5, 6. ,\n",
    "                       5. , 7. , 7.5, 8.5, 2.5, 2.5, 4. , 5.5, 6.5, 5.5, 1.5, 4.5, 6.5,\n",
    "                       5.5, 6.5, 4.5, 4.5, 5.5, 5.5, 5.5, 5. , 4. , 5.5, 5. , 7. , 7. ,\n",
    "                       5.5, 4.5, 4. , 5.5, 5. , 4.5, 5. , 4. , 6. ])\n",
    "\n",
    "\n",
    "control = np.array([ 6. ,  8.5,  7.5,  7.5,  5. ,  7. ,  8. ,  5. ,  6. ,  6.5,  4.5,\n",
    "                    7. ,  8. ,  5.5,  7.5,  4.5,  6.5,  4. ,  8.5,  7.5,  6.5,  5.5,\n",
    "                    9. , 10. ,  3.5,  9. ,  9.5,  7.5,  4.5,  8. ,  6.5,  5.5,  4.5,\n",
    "                    7.5,  8. ,  5.5,  7.5,  5. ,  8. ,  7. ,  6.5,  6.5,  8. ,  8. ,\n",
    "                    7.5,  7. ,  7. ,  7.5,  7. ,  8. ,  6. ,  2.5,  8.5, 10. ,  7.5,\n",
    "                    6.5,  6.5,  6.5,  6.5,  5. ,  6. ,  5. ,  7. ,  6. ,  7.5,  6.5,\n",
    "                    8.5,  7. , 10. ,  3.5,  5.5,  8. , 10. ,  6.5,  7. ,  6.5,  9. ,\n",
    "                    6. ,  8. ,  6. ,  4.5,  7. ,  7.5,  9. ,  6.5,  5.5,  7.5,  7. ,\n",
    "                    7. ,  6.5,  8.5,  7.5, 10. ,  8.5,  8. ,  4. ,  7.5,  7.5,  8. ,\n",
    "                    8.5])\n",
    "\n",
    "\n",
    "music = np.array([4. , 6. , 3.5, 8. , 2.5, 1.5, 2. , 3. , 3. , 8. , 4. , 2.5, 6.5,\n",
    "                  3.5, 2.5, 6. , 5.5, 4. , 7. , 5. , 7.5, 5.5, 7. , 5. , 2.5, 8.5,\n",
    "                  3.5, 4.5, 3.5, 4.5, 5. , 5. , 5.5, 7.5, 4.5, 2.5, 2.5, 3.5, 2. ,\n",
    "                  6. , 4. , 4. , 4.5, 2.5, 3. , 5.5, 5.5, 4. , 7.5, 2.5, 4. , 4. ,\n",
    "                  0. , 8. , 5. , 3. , 8. , 5.5, 5.5, 3.5, 5. , 2. , 6. , 4. , 5. ,\n",
    "                  6.5, 5. , 5. , 5. , 3.5, 6. , 7. , 6. , 6. , 3. , 3. , 2. , 6.5,\n",
    "                  4. , 7. , 3.5, 4. , 5. , 6. , 4.5, 5. , 4.5, 5. , 3. , 5. , 4.5,\n",
    "                  4.5, 5.5, 3.5, 4.5, 4. , 5.5, 6. , 3. , 5.5])\n",
    "# fmt: on\n",
    "\n",
    "pain = pd.DataFrame({\"audio_book\": audio_book, \"music\": music, \"control\": control,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame has a little different format than we've been using.  It might be just fine to continue with the analysis as is, but for extra practice, let's reformat this data to be taller.\n",
    "\n",
    "Reformat the data to have 2 columns, `'treatment'` & `'pain'`, where `'treatment'` holds the value of `['audio_book', 'music', 'control']` & `'pain'` holds the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall_pain = ____\n",
    "tall_pain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data by group somehow, which treatment group(s) appear to be the most effective at reducing pain?  Which appear to be the least effective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Assumptions\n",
    "\n",
    "From wikipedia:\n",
    "\n",
    "> * Response variable residuals are normally distributed (or approximately normally distributed).\n",
    "> * Variances of populations are equal.\n",
    "> * Responses for a given group are independent and identically distributed normal random variables.\n",
    "\n",
    "### Normality!\n",
    "\n",
    "As pointed out by this [StackExchange question](https://stats.stackexchange.com/q/60410/102646), this assumption isn't the most clearly communicated thing across the internet.\n",
    "\n",
    "[This resource](https://www.theanalysisfactor.com/checking-normality-anova-model/) lays out a nice summary to hopefully clear up some confusion.\n",
    "\n",
    "> The normality assumption is that residuals follow a normal distribution.\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> But what it’s really getting at is the distribution of Y|X.   That’s Y given the value of X.  Because X values are considered fixed, they have no distributions.  Residuals have the same distribution as Y|X.  If residuals are normally distributed, it means that Y is normally distributed within a value of X (not necessarily overall).\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> But when predictors are categorical, there are usually just a few values of X (the categories), and there are many observations at each value of X.  So you’ll often see the normality assumption for an ANOVA stated as:\n",
    ">\n",
    "> “The distribution of Y within each group is normally distributed.”  It’s the same thing as Y|X and in this context, it’s the same as saying the residuals are normally distributed.\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> So in ANOVA, you actually have two options for testing normality.  If there really are many values of Y for each value of X (each group), and there really are only a few groups (say, four or fewer), go ahead and check normality separately for each group.\n",
    ">\n",
    "> But if you have many groups (a 2x2x3 ANOVA has 12 groups) or if there are few observations per group (it’s hard to check normality on only 20 data points), it’s often easier to just use the residuals and check them all together.\n",
    "\n",
    "In this notebook, let's check both ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk\n",
    "\n",
    "From [`scipy.stats.shapiro` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html):\n",
    "\n",
    "> The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n",
    "\n",
    "* $H_o$: data was drawn from a normal distribution\n",
    "* $H_a$: data was not drawn from a normal distribution\n",
    "\n",
    "You'll often see it noted that for large sample sizes, even slight deviations from normality are detected.  Also, like the t-test, ANOVA is fairly robust to deviations from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p = ____\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p = ____\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p = ____\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QQ Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(audio_book, line=\"s\")\n",
    "plt.show()\n",
    "\n",
    "qqplot(music, line=\"s\")\n",
    "plt.show()\n",
    "\n",
    "qqplot(control, line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance!\n",
    "\n",
    "From [`scipy.stats.bartlett` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bartlett.html):\n",
    "\n",
    "> Bartlett’s test tests the null hypothesis that all input samples are from populations with equal variances.\n",
    "\n",
    "* $H_o$: all input samples are from populations with equal variances\n",
    "* $H_a$: all input samples are not from populations with equal variances\n",
    "\n",
    "Before we run a formal test, let's show the standard deviation of each group sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the more formal test.  Note, like Shapiro-Wilk, this test can also be sensitive and Levene's test is sometimes used as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a difference in level of pain reported between the groups?\n",
    "\n",
    "We have 3 groups, doing repeated t-tests would be compounding our chance of Type I error.  To combat this, we can use a one-way ANOVA.  For this, we can use `scipy` or `statsmodels`.  Below is how we can do this `scipy`, we'll dig more into `statsmodels` output/options.\n",
    "\n",
    "In the first cell of the notebook, the data is loaded into distinct variables, this is how `scipy.stats.f_oneway` is expecting the data.\n",
    "\n",
    "What do we conclude?\n",
    "\n",
    "* $H_o$: The mean is the same for all groups\n",
    "* $H_a$: The mean is not the same for all groups (doesn't say what mean(s) is/are different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, p = ____(audio_book, music, control)\n",
    "\n",
    "print(f)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `statsmodels`, we'll use the 'formula syntax' which has the basic form of:\n",
    "\n",
    "`'y ~ x'` or `'(explain this) ~ (using this)'`\n",
    "\n",
    "We want to explain pain using treatment as our explanation.  So we want to do something like `'pain ~ treatment'`.  The last piece of the puzzle is how to deal with `'treatment'` being a categorical variable.  To include that info in the formula we use `'C()'`.  So our final formula will be:\n",
    "\n",
    "`'pain ~ C(treatment)'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ____\n",
    "anova_table = sm.stats.anova_lm(lm, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `statsmodels` output we can revisit the normality assumption in light of the 'residuals'.  We'll discuss more about these when we get to linear regression.  For now we'll just think of them as a different way to check the normality assumption.\n",
    "\n",
    "Visualize the residuals to inspect normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a formal test for normality on the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our ANOVA hypothesis testing.\n",
    "\n",
    "There is a significant difference for at least one of our groups' means!\n",
    "\n",
    "Let's find out which one with ad hoc analysis.  To use the `pairwise_tukeyhsd()` function from `statsmodels` it would be simpler if our data was in long/tall format; currently our data is in a wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_results = ____\n",
    "\n",
    "tukey_results.plot_simultaneous()\n",
    "plt.show()\n",
    "\n",
    "tukey_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results show that the control group reported pain at a significantly higher level than both of our audio groups.  There was no difference found between the 2 audio groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch up the Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now say that instead of having three separate groups we only have one group of patients.  We played our subjects an audio book during a painful procedure, later we did the same painful procedure and played them nothing (our control).\n",
    "\n",
    "We do not have independent samples in this case, so we should not use our `ttest_ind` test that assumes independence.  However, like an independent t-test, a paired t-test is still the ratio of $\\frac{signal}{noise}$.\n",
    "\n",
    "#### Our signal\n",
    "\n",
    "Instead of comparing the difference of the 2 groups means (`mean(x1) - mean(x2)`), we take the difference of each pair's values and then take the mean of this set of differences (`mean(x1 - x2)`). \n",
    "\n",
    "#### Our noise\n",
    "\n",
    "In an independent t-test our noise is a calculated to be a pooled standard error. In a paired t-test since we're just looking at one value as our signal, so we'll use its standard error as the measure of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing paired t-test 'by hand'\n",
    "\n",
    "* From the original `pain` dataframe, create a new dataframe with only the `audio_book` & `control` variables\n",
    "* Create a new column that contains the difference between these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_pain = _____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check normality assumption\n",
    "\n",
    "Note, similar to an independent t-test, with larger sample sizes this test is robust to deviations from normality.  This robustness is more related to Type I error (i.e. we don't significantly increase risk of a fals positive); with violations of this assumption we might be more at risk of a Type II error (i.e. a false negative).\n",
    "\n",
    "Assess normality of the differences visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess with a more formal test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the value of `t` 'by hand'.\n",
    "\n",
    "$$t = \\frac{signal}{noise}$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$signal = \\bar{d}$$\n",
    "$$noise = \\frac{s_d}{\\sqrt{n_d}}$$\n",
    "\n",
    "And $d$ represents the differences between the paired samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = ____\n",
    "noise = ____\n",
    "\n",
    "test_statistic = signal / noise\n",
    "test_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, that's a pretty large value of $t$.  If using our handy 1.96 we would reject the null.  In practice, it's nicer to use a prewritten function that will find $p$ for us and we can just interpret that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `scipy.stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = ____\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval\n",
    "\n",
    "Like the independent t-test, our CI formula is:\n",
    "\n",
    "$$signal \\pm scalar * noise$$\n",
    "\n",
    "Where the $scalar$ will be a critical value related to our level of confidence.  In our examples, we've been sticking with 1.96, but this could be looked up more formally with `scipy.stats.t.ppf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
