{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data Analysis\n",
    "\n",
    "The data used in this notebook was downloaded from [Kaggle](https://www.kaggle.com/drgilermo/nba-players-stats#Seasons_Stats.csv).  The original source of the data is [Basketball-reference](http://www.basketball-reference.com/).\n",
    "\n",
    "\n",
    "## General Intro EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import plotly_express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/1m0jaYL1KGjxW1cKJUQxVTcPOnm7v7NZEBKRZADCmc68/export?format=csv\"\n",
    "nba = pd.read_csv(data_url, index_col=0)\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing from before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the impact for dropping NAs based on these columns is low, perform the drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = nba.drop(columns=[\"blank2\", \"blanl\"])\n",
    "nba = nba.dropna(subset=[\"Year\", \"Player\", \"Pos\", \"Tm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We have a lot of useful data here, but most predictive models that we'll be looking at only like numeric data.  To still use our information we have to do a little bit of reformatting.\n",
    "\n",
    "### One Hot Encoding / Dummy Encoding\n",
    "\n",
    "For example, for team, we might \"one-hot encode\" (aka create dummy variables).  This is a way of creating a series of variables indicating True/False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe that is a subset of the `nba` dataframe.  Only include in this subset:\n",
    "\n",
    "* Columns: `PTS`, `Player`, & `Tm`\n",
    "* Rows: a random selection of 15 rows (use 42 as the `random_state`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# subset columns\n",
    "nba_sub = ____\n",
    "\n",
    "# subset rows\n",
    "nba_sub = ____\n",
    "\n",
    "nba_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pd.get_dummies()` on the subset.\n",
    "\n",
    "* What happened?\n",
    "* What might we change about this and why?\n",
    "* What does the `drop_first` argument of `pd.get_dummies()` do and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues that come up with using `pd.get_dummies` in a machine learning workflow.  For today, we'll stick with it due to its ease of use compared to more powerful options.\n",
    "\n",
    "Using [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) overcomes the issues that `pd.get_dummies` can run into, but it has a little more complex usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary column named `is_old` that shows whether or not the `Year` variable is before 1980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary column named `is_california` that shows whether or not a team is located in california."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_teams = [\"LAL\", \"LAC\", \"GSW\", \"SAC\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up some data to be ordinal encoded.\n",
    "\n",
    "* Using the `grades` list create a sample of 20 random letters\n",
    "* Create a 1 column DataFrame from this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "grades = [\"A\", \"B\", \"C\", 'D', 'F']\n",
    "rand_grades = ____\n",
    "\n",
    "grade_df = ____\n",
    "grade_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable that is an ordinal encoding of grade.  Have `A` be 1 and `F` be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Some methods we'll see are sensitive to our variables being on different scales.  For example, if you have variables for a person's height and their annual income, the height feature will have a much much smaller value than the income feature.  In some methods, this will lead to the income variable being a louder signal than the height variable.  Larger magnitude variables can end up drowning out smaller magnitude ones, and this can be an issue if we think height will be an important predictor.\n",
    "\n",
    "To address this issue, we can scale the variables to have equal footing.  This won't change the shape of their distribution; this shape not changing means that the patterns within the variable aren't lost by scaling, the patterns are preserved, the values have just been standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a subset of the nba dataset that has the columns `PTS` and `Age`.\n",
    "* Drop all NAs\n",
    "* Use the pandas boxplot method on this resulting data.\n",
    "* Plot these variables on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data_scientists = [\"Adam\", \"Emily\", \"Heather\", \"Hunter\",\n",
    "                   \"Jon\", \"Michael\", \"Shobair\", \"Wyatt\"]\n",
    "# fmt: on\n",
    "\n",
    "# Randomize order\n",
    "np.random.shuffle(data_scientists)\n",
    "\n",
    "n = len(data_scientists) // 2\n",
    "print(f\"Use StandardScaler: {data_scientists[:n]}\")\n",
    "print(f\"Use MinMaxScaler: {data_scientists[n:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick your poison\n",
    "scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a scaler to scale the `PTS` and `Age` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# .fit() methods 'learn' something from your data\n",
    "# They don't apply any of these learnings\n",
    "# In the case of a scaler we have to call .transform\n",
    "# Alternatively, we could use .fit_transform() to do \n",
    "# both of these things in one step\n",
    "scaler.fit(nba_sub)\n",
    "\n",
    "scaled = scaler.transform(nba_sub)\n",
    "print(type(scaled))\n",
    "print(scaled.shape)\n",
    "print(scaled)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled, columns=nba_sub.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Redo the boxplot from before\n",
    "* Redo the scatterplot from before\n",
    "\n",
    "What do you see? What's changed? What hasn't changed?\n",
    "\n",
    "* Bonus: what attributes does your scaler have? What is the significance of these?\n",
    "* Bonus Bonus: can you recreate this same scaling from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
