{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_moons(n_samples=5000, noise=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataframe from our `X` and `y` components.  Name the columns `x1`, `x2`, and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatterplot of `x1` x `x2` colored by `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a train/test split (use the original `X` and `y` for simplicity)\n",
    "* Use 20% of the data for testing\n",
    "* Stratify the split by your class labels to ensure equal proportions of both labels in train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "* Build an SVM classifier with a linear kernel\n",
    "* Print the train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_train, y_train, clf=model, scatter_kwargs={\"alpha\": 0.05})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM\n",
    "\n",
    "* Build an SVM classifier with a poly kernel\n",
    "* Print the train and test accuracy\n",
    "* Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _____\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Train score: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "plot_decision_regions(X_train, y_train, clf=model, scatter_kwargs={\"alpha\": 0.05})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through varying degrees of polynomials and show the accuracy/plot for each.  This could be a use case for a function.  Because we want to redo the same process over and over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside about `kwargs`.  This abbrevation stands for **K**ey **W**ord **ARG**ument**S**.  If you want your function to accept a lot of different arguments, but you don't want to limit them you might use `kwargs`.  In this case of our function.  We want to pass keyword arguments to the model.  Inside the function, we'll print these out and see that they end up as a dictionary in the function.  We then use a double asterisk to pass the functions to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def svm_fit_score_plot(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "    model = SVC(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\\nTrain score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Test score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "    plot_decision_regions(X_train, y_train, clf=model, scatter_kwargs={\"alpha\": 0.05})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `for` loop to pass each value of `degrees` to our custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degrees = [1, 2, 5, 10]\n",
    "for ______:\n",
    "    svm_fit_score_plot(kernel=\"poly\", degree=_____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't care about plotting a better solution than our function would be to use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from `sklearn`.\n",
    "\n",
    "* Define a dictionary named `grid` with the key as `\"degree\"` and the value as a list of values: 1, 2, & 5.\n",
    "* Create a `GridSearchCV` object with `SVC` using the `\"poly\"` `kernel`\n",
    "* Fit the model to your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = _____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `verbose` option we see that a total of `15` models were fit.  This number is specifically from the `Fitting 5 folds for each of 3 candidates` verbage.  This means we did 3 different models (the number of degree options we gave the model) and we fit each of these models 5 times (1 per fold, aka a new train/test split 5 times).\n",
    "\n",
    "The parameters that led to the average accuracy on each of these test sets is stored in the `best_params_` attribute.  In this case, we see that a polynomial of degree 1 is the most accurate. A polynomial of degree one is a line.  So all we did was a fancy linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know of a another hyperparameter though, `C`.   We can add this to our `GridSearchCV` to evaluate the best combination of `C` and `degree`.  This adds a lot more models to build.  This is something to consider when grid searching.  We added 3 values for `C` so this triples the number of models to be fit (1 new model for each combination of `degree` and `C`.\n",
    "\n",
    "* Add `C` to our parameters dictionary with the values `0.1`, `1`, `10`\n",
    "* Re-run the grid search with these parameter options\n",
    "* Print out the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"degree\": [1, 2, 5], \"C\": [0.1, 1, 10]}\n",
    "\n",
    "model = GridSearchCV(SVC(kernel=\"poly\"), params, verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, we're not linear anymore.  Let's view this new decision boundary. With our custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = model_cv.best_params_[\"C\"]\n",
    "degree = model_cv.best_params_[\"degree\"]\n",
    "\n",
    "svm_fit_score_plot(kernel=\"poly\", C=C, degree=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF SVM (radial basis function)\n",
    "\n",
    "* Build an SVM classifier with a rbf kernel\n",
    "* Print the train and test accuracy\n",
    "* Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Train score: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "plot_decision_regions(X_train, y_train, clf=model, scatter_kwargs={\"alpha\": 0.05})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooh, that's pretty and it works pretty well in comparison to other types.  Let's vary the value of `C` and see what happens.\n",
    "\n",
    "* Define a list of c values using `0.1`, `1`, `10`, `100`\n",
    "* Write a `for` loop to pass each of these values to our custom function (use the rbf kernel in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of our decision regions changes pretty drastically, but our accuracy doesn't.  This is where the principle of parsimony should come into play.  The simpler the model the better.  `C` is essentially a measure of how complex the model is.  So if you have models with similar accuracy, in general, you should choose the simpler model (in this case we would choose the lower value of `C` unless a higher value shows to perform way better).\n",
    "\n",
    "## Grid Search all the things\n",
    "\n",
    "What's the best `kernel`, `degree`, `C`?  We've seen a grid search for `degree` and `C`.  We can add `kernel` to this search as well.\n",
    "\n",
    "* Add `kernel` to our grid search `params` dictionary; use every kernel we've looked at in this notebook\n",
    "* Perform the grid search\n",
    "* Print the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "135 models! That's a lot of work being done for us.  What's the best model with these parameter options?\n",
    "\n",
    "* Pass in the best parameters to our custom function to `score` it and view the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
