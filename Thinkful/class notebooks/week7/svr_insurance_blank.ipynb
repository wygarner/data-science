{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Insurance prices üí∞\n",
    "\n",
    "Read more about dataset from [this kaggle page](https://www.kaggle.com/mirichoi0218/insurance).  The original source of the data is not mentioned in the description :(\n",
    "\n",
    "We'll be predicting charges based on some features of the clients.\n",
    "\n",
    "## General EDA and Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "insurance = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some general eda to get familiar with what data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our visualization journey with a `scatter_matrix` or an `sns.pairplot` to see all our continuous variables.  Categorical variables we might plot individually with our target since we have a managable number of columns.\n",
    "\n",
    "* Create a `scatter_matrix` or an `sns.pairplot` of the data\n",
    "* Note any correlations, patterns, and distributions you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an interesting pattern in the scatterplot between `'age'` and `'charges'`.  It looks like there's 3 distinct groups.  This is hinting that there might be some variable interacting with `'age'`.  That is, the effect of `'age'` on `'charges'` might change based on some other variable at play.\n",
    "\n",
    "Hopefully we can find this interacting variable in our data.  However, it might be explained by a variable outside of our dataset; some domain knowledge would then be very useful to try and bring what explains these gaps.\n",
    "\n",
    "Let's visualize our categorical variables and try to figure out what might explain this.\n",
    "\n",
    "* Create a for loop that iterates over all of the categorical columns\n",
    "* Create a violin plot for each column with our `'charges'` column\n",
    "* Interpret what you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['smoker', 'sex', 'region']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Smokers definitely appear to be charged more than non-smokers\n",
    "* The distributions between the sexes are pretty similiar.  The main difference is that the male distribution has a 'fatter' right hand tail (i.e. a group of males getting charged more)\n",
    "* Region seems to have a very small effect on charges.  Perhaps the southeast has a fatter tail than the others, but this may be less due to region itself, and more explained by other variables.  For example, I think I've read that the southeast has some of the highest rates of obesitity, idk if this is still accurate (or if it ever was)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw some effects that might help us explain the weird patterns shown in the `'age'` x `'charges'` scatter plot.  Let's try to include smoker in a scatterplot with these 2 variables since it appears to have the most extreme effect.\n",
    "\n",
    "* Create a `scatterplot` of `'age'` and `'charges'` colored by the `'smoker'` column\n",
    "* What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like being a smoker interacts with age to explain part of this pattern of groups.  The effect of age on charges changes depending on whether or not the individual is a smoker.\n",
    "\n",
    "This plot shows that being a smoker could explain the top and bottom groups, but what about the middle group?  It looks like there's still a pattern to be found.\n",
    "\n",
    "The remaining bit of the pattern is a little bit harder to discover.  We're already showing 3 dimensions in our 2d plot thanks to color.  We could include a sizing dimension to try and bring in a 4th dimension, but this is harder to interpret.\n",
    "\n",
    "* Add the `'bmi'` column to the scatter plot using the `size` argument of `sns.scatterplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting `'bmi'` looks to explain away some of the pattern as well. It's less obvious, but it looks like lower bmi smokers are in the middle group, while higher bmi smokers are in the top group.  There are exceptions to this though.  If I was a data scientist working on this problem I would want to discuss this plot with my clients to make sure I have all the features that account for these 3 'strips' of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep\n",
    "\n",
    "We'll keep all our features for now.  We might consider dropping region since it seems to have little effect based on our plots, but we would want to consult with experts (or build model with and without it and see if it has a positive effect on the accuracy or if its just dead weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"sex\", \"smoker\", \"region\"]\n",
    "# Dropping southeast dummy since its the most occuring category\n",
    "# So from that I'm assuming it might be a good 'default'\n",
    "drop_cats = [\"male\", \"no\", \"southeast\"]\n",
    "\n",
    "num_cols = [\"age\", \"bmi\", \"children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = insurance.drop(columns=\"charges\")\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finish the `ColumnTransformer()` definition by adding a `StandardScaler()` that will be applied to the columns listed in the `num_cols` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"encode_cats\", OneHotEncoder(drop=drop_cats), cat_cols),\n",
    "        ______\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finish the `PipeLine()` definition by adding an `SVR()` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        _______\n",
    "    ],\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the pipeline to the training data\n",
    "* Score the model on train and testing data\n",
    "    * What is this score?\n",
    "    * Do we want to minimize it or maximize it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____\n",
    "\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"train_score: {train_score}\")\n",
    "print(f\"test_score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang, that's horrible... Maybe we want a different `kernel`, maybe we want a different `C`, a different `epsilon`. Sounds easier to `GridSearchCV` for this than to do a lot of guess and check.\n",
    "\n",
    "* Define a dictionary of parameters to be our grid to search through\n",
    "    * Use `0.1`, `1`, & `10` for the potential values of `'epsilon'`\n",
    "      * To specify parameters in a pipeline we have to provide the name of the step they apply to.  We specify the name of the step and then 2 underscores before using the usual hyperparameter name.  For example, if you named the `SVR()` step in your `Pipeline` `'svm'`, then the parameter name for `'epsilon'` would be `'svm__epsilon'`.\n",
    "    * Try some of the kernels\n",
    "    * Try a couple different degree polynomials\n",
    "    * Note how many models you'll be building before running the grid search\n",
    "* Define the grid search by passing in the pipeline, parameter grid\n",
    "* Fit the grid search.\n",
    "* Print out the best parameters.\n",
    "    * Remember that `degree` is ignored by all `kernel`s except `'poly'`.  So if the best kernel isn't `'poly'` we can ignore the `degree` parameter chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"svm__epsilon\": [0.1, 1, 10],\n",
    "    ____,\n",
    "    ____,\n",
    "    ____,\n",
    "}\n",
    "\n",
    "pipeline_cv = GridSearchCV(____, ____, verbose=1)\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "print('Best hyperparameters:')\n",
    "pipeline_cv._____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Score the resulting model that was fit via grid search cross validation\n",
    "* If the fit is poor, take note of what hyperparameters were chosen, are any of them at the edges of the grid?  Adjusting the grid might improve fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = pipeline_cv.score(X_train, y_train)\n",
    "test_score = pipeline_cv.score(X_test, y_test)\n",
    "\n",
    "print(f\"train_score: {train_score}\")\n",
    "print(f\"test_score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make predictions on the test set\n",
    "* Calculate the mean absolute error (MAE) and the mean absolute percent error (MAPE)\n",
    "* Create a plot of the fitted vs the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
