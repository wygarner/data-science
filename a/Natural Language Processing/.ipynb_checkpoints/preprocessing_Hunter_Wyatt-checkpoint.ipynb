{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iCUbt7u4JIO"
   },
   "source": [
    "# Text Data Cleaning and Preprocessing Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bY9e9TNNn5et",
    "outputId": "cf5a506b-0b90-4d27-944d-ae6331d937f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JfE1hyh_qktd",
    "outputId": "3c5dd7f4-abbf-49af-98d4-020c24e0b992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "OhAy4kaurnzI",
    "outputId": "e3293a73-f81a-43e2-8012-5a26b4706463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XgryLFFclzWc",
    "outputId": "6ed03663-7a90-4529-c714-8eabbe96b93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuqMJsi-4JIS"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCY5p4L54JIY"
   },
   "source": [
    "### Read the O'Reilly RSS plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmNkoVRZ4JIc"
   },
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'.*\\.txt'\n",
    "corpus = PlaintextCorpusReader('/content/drive/My Drive/Colab Notebooks/Assignments/PairProgramming/NLP/oreilly', DOC_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJI6AbGA4JIg"
   },
   "source": [
    "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NwktonOxm2RO",
    "outputId": "99134dd6-32b1-4e81-b450-66b3cfbc0a4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most of the technical news this month continues to swirl around coronavirus. Many things are happening under that rubric\\u2060—for example, delivery during a pandemic could be the killer app for autonomous vehicles. And money being (literally) dirty, the pandemic could drive the development of new payment systems that are more inclusive. I haven’t seen as much interesting news from the AI world this month.\\xa0 There are some hints that we’re reaching the point of diminishing returns for large models, and that we’re turning the corner from research into deployment and production. COVID-19 A startup is developing a CRISPR-based fast and accurate COVID-19 test that can be used at home. At-home testing raises questions about reporting, but since the test gives quick results, it could also be used by airlines, offices, and other crowded locations. One consequence of the coronavirus pandemic may be that there aren’t enough people to work in call centers.\\xa0 Will AI take up the slack?\\xa0 Natural language applications for automating call centers is a significant strategic bet for a number of big AI players, including IBM.  Coronavirus may be a boon for autonomous vehicles: we may soon see driverless delivery services and driverless taxis, both of which take the driver out of the infection disease cycle.\\xa0 CVS is testing a self-driving prescription delivery service. Permanent work-from-home at Twitter: On one hand, working from home wherever possible is a no-brainer, if for no other reason than minimizing office expense. But it has huge consequences for the real-estate and building industries if other companies follow suit. It also may have consequences for salaries. Covid Tracker Tracking: Quis custodiet ipsos custodes (Who watches the watchers)?\\xa0 MIT, evidently. The startup Miso has developed a tool for doing smart, AI-based searches on COVID-19 research.\\xa0 A number of similar tools are appearing, including Google’s COVID-19 Research Explorer and SciFact from the Allen Institute. Given the number of research papers that have been generated, AI-based search and summarization may be the only way to stay current. In the wake of conference cancellations, venues for online conferences and meetings are evolving rapidly; it’s not just Zoom or GoToMeeting. Discord has a lot of potential; The Deserted Island Devops conference appears to have taken place in Animal Crossing, a multiplayer video game.\\xa0 The conference was live-streamed on Twitch. I’ve also seen references to The Online Town and Clubhouse Voice Chat. The winner will be whoever can replicate the in-person “hallway session” experience. Will the open office survive COVID-19?\\xa0 (Personal conversation) Good question. Open offices have already lost favor, at least among employees. Viruses spread like wildfire in open spaces with shared ventilation. Keeping office buildings at 25% capacity isn’t attractive–and if HVAC is a big part of the problem, unlikely to be effective. Programming NuShell is a new cross-platform shell, updating the notion of pipes and built around tables as first-class objects. It runs on Windows, Linux, and OS X.\\xa0 The command line will remain a powerful tool. Bosque is Microsoft’s new programming language for the cloud and AI. It strikes me as a weird mashup of JavaScript and TypeScript with some Java-esque features; Microsoft stresses Bosque’s intermediate representation (bytecode), which is a level of arcanity I don’t expect anyone to care about. Deno, a new command-line JavaScript interpreter, is attracting a lot of attention. If this seems like “an updated version of Node.js that fixes the mistakes we made the first time,” I’m not sure why it isn’t.\\xa0 Perhaps more important, Deno includes a TypeScript interpreter, eliminating cross-compilation from TypeScript to JavaScript. The New Stack is talking about Serverless Cloud mashups. This is also the point of James Urquhart’s “flow architectures,” which I suspect has been missed: what James thinks is radical about the next generation of enterprise software is that it will be as flexible as the web. Machine Learning and AI Comment-driven development: Microsoft has developed AI that writes functions based on a comment and the function signature (skip to 29:00 of the video). This is obviously a demo with extremely simple examples, but this could be a big step forward in re-thinking programming.\\xa0 Programming would be less about describing processes in excruciating detail, and more about analyzing the problem itself. Early Bird is a technique for faster and less power-hungry training for neural networks by doing network pruning (identifying the nodes that will be part of the trained network) very early in the process.\\xa0 They claim to use a factor of 10 less energy. There are two important questions: does that just mean that people will train larger models? And do larger models actually accomplish anything? The natural language model GPT-3, which is the largest model we know of, suggests that we’ve reached a point of diminishing returns. And that we’re asking the wrong questions, particularly for language models. PyCaret is a new machine learning library for Python that requires very little coding. Payment systems Affirm (buy-now-pay later, essentially a one-shot credit card) seems to have flown under the radar. Credit is only extended for a single online purchase, with credit-worthiness and loan terms evaluated in real-time. Interest rates are based on your ability to pay: ⅓ of the loans are at 0%, but can be up to 30%, which seems usurious. Users are given the total payments and interest charges before they accept the loan. Payment as a microservice… The Mojaloop Foundation, with extensive support from Google and the Gates Foundation, is attempting to create a digital payments system with inclusion as a goal. This could be seen as competition for Facebook’s Libra\\u2060—although Libra appears to be dead before it started.  As long as we’re on payments: will the dollar continue to dominate?\\xa0 China continues to work on their electronic currency (the e-RMB), which could become a compelling alternative to the dollar. Networks The Open RAN Policy Coalition is attempting to create open, interoperable standards for 5G. Such a coalition is needed because the major networks and vendors are all pursuing slightly different versions of a standard that isn’t really standard. The coalition includes Microsoft, Google, IBM, Cisco, AT&T, and Verizon. It may also be an attempt to exclude China, since Chinese companies are not represented. '"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.raw('0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL7Cjcby4JIi"
   },
   "outputs": [],
   "source": [
    "big_list = []\n",
    "\n",
    "for item in corpus.fileids():\n",
    "  big_list.append(corpus.raw(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kkaqsxxRnbaS",
    "outputId": "b643cd24-d9b4-4495-9607-335d8e44466a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7JuwNTW4JIn"
   },
   "source": [
    "### Sentence tokenize each document in the list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zbqurmN4JIp"
   },
   "outputs": [],
   "source": [
    "tokenized_list = []\n",
    "\n",
    "for doc in big_list:\n",
    "  tokenized_list.append(sent_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kGsQsZhmolWQ",
    "outputId": "b88c1066-8273-4a4e-c8a6-ba832bd6bbf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Most of the technical news this month continues to swirl around coronavirus.',\n",
       " 'Many things are happening under that rubric\\u2060—for example, delivery during a pandemic could be the killer app for autonomous vehicles.',\n",
       " 'And money being (literally) dirty, the pandemic could drive the development of new payment systems that are more inclusive.',\n",
       " 'I haven’t seen as much interesting news from the AI world this month.',\n",
       " 'There are some hints that we’re reaching the point of diminishing returns for large models, and that we’re turning the corner from research into deployment and production.',\n",
       " 'COVID-19 A startup is developing a CRISPR-based fast and accurate COVID-19 test that can be used at home.',\n",
       " 'At-home testing raises questions about reporting, but since the test gives quick results, it could also be used by airlines, offices, and other crowded locations.',\n",
       " 'One consequence of the coronavirus pandemic may be that there aren’t enough people to work in call centers.',\n",
       " 'Will AI take up the slack?',\n",
       " 'Natural language applications for automating call centers is a significant strategic bet for a number of big AI players, including IBM.',\n",
       " 'Coronavirus may be a boon for autonomous vehicles: we may soon see driverless delivery services and driverless taxis, both of which take the driver out of the infection disease cycle.',\n",
       " 'CVS is testing a self-driving prescription delivery service.',\n",
       " 'Permanent work-from-home at Twitter: On one hand, working from home wherever possible is a no-brainer, if for no other reason than minimizing office expense.',\n",
       " 'But it has huge consequences for the real-estate and building industries if other companies follow suit.',\n",
       " 'It also may have consequences for salaries.',\n",
       " 'Covid Tracker Tracking: Quis custodiet ipsos custodes (Who watches the watchers)?',\n",
       " 'MIT, evidently.',\n",
       " 'The startup Miso has developed a tool for doing smart, AI-based searches on COVID-19 research.',\n",
       " 'A number of similar tools are appearing, including Google’s COVID-19 Research Explorer and SciFact from the Allen Institute.',\n",
       " 'Given the number of research papers that have been generated, AI-based search and summarization may be the only way to stay current.',\n",
       " 'In the wake of conference cancellations, venues for online conferences and meetings are evolving rapidly; it’s not just Zoom or GoToMeeting.',\n",
       " 'Discord has a lot of potential; The Deserted Island Devops conference appears to have taken place in Animal Crossing, a multiplayer video game.',\n",
       " 'The conference was live-streamed on Twitch.',\n",
       " 'I’ve also seen references to The Online Town and Clubhouse Voice Chat.',\n",
       " 'The winner will be whoever can replicate the in-person “hallway session” experience.',\n",
       " 'Will the open office survive COVID-19?',\n",
       " '(Personal conversation) Good question.',\n",
       " 'Open offices have already lost favor, at least among employees.',\n",
       " 'Viruses spread like wildfire in open spaces with shared ventilation.',\n",
       " 'Keeping office buildings at 25% capacity isn’t attractive–and if HVAC is a big part of the problem, unlikely to be effective.',\n",
       " 'Programming NuShell is a new cross-platform shell, updating the notion of pipes and built around tables as first-class objects.',\n",
       " 'It runs on Windows, Linux, and OS X.',\n",
       " 'The command line will remain a powerful tool.',\n",
       " 'Bosque is Microsoft’s new programming language for the cloud and AI.',\n",
       " 'It strikes me as a weird mashup of JavaScript and TypeScript with some Java-esque features; Microsoft stresses Bosque’s intermediate representation (bytecode), which is a level of arcanity I don’t expect anyone to care about.',\n",
       " 'Deno, a new command-line JavaScript interpreter, is attracting a lot of attention.',\n",
       " 'If this seems like “an updated version of Node.js that fixes the mistakes we made the first time,” I’m not sure why it isn’t.',\n",
       " 'Perhaps more important, Deno includes a TypeScript interpreter, eliminating cross-compilation from TypeScript to JavaScript.',\n",
       " 'The New Stack is talking about Serverless Cloud mashups.',\n",
       " 'This is also the point of James Urquhart’s “flow architectures,” which I suspect has been missed: what James thinks is radical about the next generation of enterprise software is that it will be as flexible as the web.',\n",
       " 'Machine Learning and AI Comment-driven development: Microsoft has developed AI that writes functions based on a comment and the function signature (skip to 29:00 of the video).',\n",
       " 'This is obviously a demo with extremely simple examples, but this could be a big step forward in re-thinking programming.',\n",
       " 'Programming would be less about describing processes in excruciating detail, and more about analyzing the problem itself.',\n",
       " 'Early Bird is a technique for faster and less power-hungry training for neural networks by doing network pruning (identifying the nodes that will be part of the trained network) very early in the process.',\n",
       " 'They claim to use a factor of 10 less energy.',\n",
       " 'There are two important questions: does that just mean that people will train larger models?',\n",
       " 'And do larger models actually accomplish anything?',\n",
       " 'The natural language model GPT-3, which is the largest model we know of, suggests that we’ve reached a point of diminishing returns.',\n",
       " 'And that we’re asking the wrong questions, particularly for language models.',\n",
       " 'PyCaret is a new machine learning library for Python that requires very little coding.',\n",
       " 'Payment systems Affirm (buy-now-pay later, essentially a one-shot credit card) seems to have flown under the radar.',\n",
       " 'Credit is only extended for a single online purchase, with credit-worthiness and loan terms evaluated in real-time.',\n",
       " 'Interest rates are based on your ability to pay: ⅓ of the loans are at 0%, but can be up to 30%, which seems usurious.',\n",
       " 'Users are given the total payments and interest charges before they accept the loan.',\n",
       " 'Payment as a microservice… The Mojaloop Foundation, with extensive support from Google and the Gates Foundation, is attempting to create a digital payments system with inclusion as a goal.',\n",
       " 'This could be seen as competition for Facebook’s Libra\\u2060—although Libra appears to be dead before it started.',\n",
       " 'As long as we’re on payments: will the dollar continue to dominate?',\n",
       " 'China continues to work on their electronic currency (the e-RMB), which could become a compelling alternative to the dollar.',\n",
       " 'Networks The Open RAN Policy Coalition is attempting to create open, interoperable standards for 5G.',\n",
       " 'Such a coalition is needed because the major networks and vendors are all pursuing slightly different versions of a standard that isn’t really standard.',\n",
       " 'The coalition includes Microsoft, Google, IBM, Cisco, AT&T, and Verizon.',\n",
       " 'It may also be an attempt to exclude China, since Chinese companies are not represented.']"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqT7V6zZ4JIu"
   },
   "source": [
    "### Word tokenize each sentence within each document.\n",
    "\n",
    "You should end up with a nested list structure where the outer list contains all the sentences in each document and the inner list contains the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17JMEoHe4JIv"
   },
   "outputs": [],
   "source": [
    "outer_list = []\n",
    "\n",
    "for doc in tokenized_list:\n",
    "  doc_list = []\n",
    "  for sentence in doc:\n",
    "    doc_list.append(word_tokenize(sentence))\n",
    "\n",
    "  outer_list.append(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "3Cg7eRY_pamp",
    "outputId": "0ee75134-2ae8-4595-be6b-a8dd6069bb5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'news',\n",
       " 'this',\n",
       " 'month',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'swirl',\n",
       " 'around',\n",
       " 'coronavirus',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_list[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk1r_y4C4JIz"
   },
   "source": [
    "### Tag each token with its part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Flze4fS-4JI0"
   },
   "outputs": [],
   "source": [
    "part_of_speech = []\n",
    "\n",
    "for doc in outer_list:\n",
    "  doc_list = []\n",
    "  for tokenized_sentence in doc:\n",
    "    doc_list.append(pos_tag(tokenized_sentence))\n",
    "\n",
    "  part_of_speech.append(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "W66AAhztqpMQ",
    "outputId": "a1dd4bc5-4096-4551-c684-a996bb7eb88e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Most', 'JJS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('technical', 'JJ'),\n",
       " ('news', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('month', 'NN'),\n",
       " ('continues', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('swirl', 'VB'),\n",
       " ('around', 'RP'),\n",
       " ('coronavirus', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_of_speech[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmIltOA44JI5"
   },
   "source": [
    "### Word tokenize the raw text of each document and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVUcQFkJ4JI7"
   },
   "outputs": [],
   "source": [
    "no_stops = []\n",
    "\n",
    "for document in big_list:\n",
    "  no_stopwords = [token.lower() for token in word_tokenize(document) if not token.lower() in stopwords.words('english')]\n",
    "  no_stops.append(no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Nj7s4jfPrs95",
    "outputId": "2fd2e65c-6c36-4b64-83db-e57b9fe69ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technical',\n",
       " 'news',\n",
       " 'month',\n",
       " 'continues',\n",
       " 'swirl',\n",
       " 'around',\n",
       " 'coronavirus',\n",
       " '.',\n",
       " 'many',\n",
       " 'things']"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stops[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAZggM574JI_"
   },
   "source": [
    "### For every document, stem all the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgLS7PPD4JJB"
   },
   "outputs": [],
   "source": [
    "stemmed = []\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "for document in outer_list:\n",
    "  doc_list = []\n",
    "  for sentence in document:\n",
    "    stemmed_sentence = [stemmer.stem(token) for token in sentence]\n",
    "    doc_list.append(stemmed_sentence)\n",
    "  \n",
    "  stemmed.append(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "74mBnumDsiWn",
    "outputId": "ca9c1dfa-6a28-4202-fea0-ad58cd842550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'may', 'also', 'be', 'an', 'attempt', 'to', 'exclud', 'china', ',']"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2N4mHrZ4JJQ"
   },
   "source": [
    "### Iterate through each document, computing and printing the following document statistics for each.\n",
    "\n",
    "- Number of sentences\n",
    "- Average words per sentence\n",
    "- Vocabulary\n",
    "- Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Tfsq_LkYtKqR",
    "outputId": "3990aa79-62ba-40ce-be61-7bcbc0fb468a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document #0\n",
      "Number of sentences:  62\n",
      "Avg words per sentence:  19.53\n",
      "Unique words (vocabulary:  538\n",
      "Lexical diversity:  0.44\n",
      "\n",
      "Document #1\n",
      "Number of sentences:  14\n",
      "Avg words per sentence:  22.93\n",
      "Unique words (vocabulary:  189\n",
      "Lexical diversity:  0.59\n",
      "\n",
      "Document #2\n",
      "Number of sentences:  115\n",
      "Avg words per sentence:  26.77\n",
      "Unique words (vocabulary:  1004\n",
      "Lexical diversity:  0.33\n",
      "\n",
      "Document #3\n",
      "Number of sentences:  13\n",
      "Avg words per sentence:  29.0\n",
      "Unique words (vocabulary:  226\n",
      "Lexical diversity:  0.6\n",
      "\n",
      "Document #4\n",
      "Number of sentences:  6\n",
      "Avg words per sentence:  43.83\n",
      "Unique words (vocabulary:  177\n",
      "Lexical diversity:  0.67\n",
      "\n",
      "Document #5\n",
      "Number of sentences:  12\n",
      "Avg words per sentence:  24.75\n",
      "Unique words (vocabulary:  175\n",
      "Lexical diversity:  0.59\n",
      "\n",
      "Document #6\n",
      "Number of sentences:  8\n",
      "Avg words per sentence:  19.62\n",
      "Unique words (vocabulary:  100\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #7\n",
      "Number of sentences:  16\n",
      "Avg words per sentence:  24.06\n",
      "Unique words (vocabulary:  204\n",
      "Lexical diversity:  0.53\n",
      "\n",
      "Document #8\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  28.78\n",
      "Unique words (vocabulary:  151\n",
      "Lexical diversity:  0.58\n",
      "\n",
      "Document #9\n",
      "Number of sentences:  230\n",
      "Avg words per sentence:  24.46\n",
      "Unique words (vocabulary:  1272\n",
      "Lexical diversity:  0.23\n",
      "\n",
      "Document #10\n",
      "Number of sentences:  19\n",
      "Avg words per sentence:  17.37\n",
      "Unique words (vocabulary:  192\n",
      "Lexical diversity:  0.58\n",
      "\n",
      "Document #11\n",
      "Number of sentences:  111\n",
      "Avg words per sentence:  27.01\n",
      "Unique words (vocabulary:  917\n",
      "Lexical diversity:  0.31\n",
      "\n",
      "Document #12\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  20.44\n",
      "Unique words (vocabulary:  117\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #13\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  32.0\n",
      "Unique words (vocabulary:  148\n",
      "Lexical diversity:  0.66\n",
      "\n",
      "Document #14\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  18.57\n",
      "Unique words (vocabulary:  94\n",
      "Lexical diversity:  0.72\n",
      "\n",
      "Document #15\n",
      "Number of sentences:  119\n",
      "Avg words per sentence:  27.11\n",
      "Unique words (vocabulary:  869\n",
      "Lexical diversity:  0.27\n",
      "\n",
      "Document #16\n",
      "Number of sentences:  6\n",
      "Avg words per sentence:  23.0\n",
      "Unique words (vocabulary:  96\n",
      "Lexical diversity:  0.7\n",
      "\n",
      "Document #17\n",
      "Number of sentences:  10\n",
      "Avg words per sentence:  21.6\n",
      "Unique words (vocabulary:  132\n",
      "Lexical diversity:  0.61\n",
      "\n",
      "Document #18\n",
      "Number of sentences:  12\n",
      "Avg words per sentence:  24.08\n",
      "Unique words (vocabulary:  168\n",
      "Lexical diversity:  0.58\n",
      "\n",
      "Document #19\n",
      "Number of sentences:  109\n",
      "Avg words per sentence:  33.1\n",
      "Unique words (vocabulary:  1059\n",
      "Lexical diversity:  0.29\n",
      "\n",
      "Document #20\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  26.71\n",
      "Unique words (vocabulary:  133\n",
      "Lexical diversity:  0.71\n",
      "\n",
      "Document #21\n",
      "Number of sentences:  10\n",
      "Avg words per sentence:  18.3\n",
      "Unique words (vocabulary:  124\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #22\n",
      "Number of sentences:  45\n",
      "Avg words per sentence:  19.6\n",
      "Unique words (vocabulary:  436\n",
      "Lexical diversity:  0.49\n",
      "\n",
      "Document #23\n",
      "Number of sentences:  10\n",
      "Avg words per sentence:  21.3\n",
      "Unique words (vocabulary:  142\n",
      "Lexical diversity:  0.67\n",
      "\n",
      "Document #24\n",
      "Number of sentences:  13\n",
      "Avg words per sentence:  19.31\n",
      "Unique words (vocabulary:  152\n",
      "Lexical diversity:  0.61\n",
      "\n",
      "Document #25\n",
      "Number of sentences:  18\n",
      "Avg words per sentence:  24.89\n",
      "Unique words (vocabulary:  245\n",
      "Lexical diversity:  0.55\n",
      "\n",
      "Document #26\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  27.57\n",
      "Unique words (vocabulary:  134\n",
      "Lexical diversity:  0.69\n",
      "\n",
      "Document #27\n",
      "Number of sentences:  97\n",
      "Avg words per sentence:  20.84\n",
      "Unique words (vocabulary:  667\n",
      "Lexical diversity:  0.33\n",
      "\n",
      "Document #28\n",
      "Number of sentences:  15\n",
      "Avg words per sentence:  22.0\n",
      "Unique words (vocabulary:  189\n",
      "Lexical diversity:  0.57\n",
      "\n",
      "Document #29\n",
      "Number of sentences:  10\n",
      "Avg words per sentence:  19.3\n",
      "Unique words (vocabulary:  130\n",
      "Lexical diversity:  0.67\n",
      "\n",
      "Document #30\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  23.86\n",
      "Unique words (vocabulary:  125\n",
      "Lexical diversity:  0.75\n",
      "\n",
      "Document #31\n",
      "Number of sentences:  8\n",
      "Avg words per sentence:  21.5\n",
      "Unique words (vocabulary:  117\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #32\n",
      "Number of sentences:  8\n",
      "Avg words per sentence:  33.38\n",
      "Unique words (vocabulary:  168\n",
      "Lexical diversity:  0.63\n",
      "\n",
      "Document #33\n",
      "Number of sentences:  11\n",
      "Avg words per sentence:  21.64\n",
      "Unique words (vocabulary:  153\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #34\n",
      "Number of sentences:  74\n",
      "Avg words per sentence:  22.26\n",
      "Unique words (vocabulary:  545\n",
      "Lexical diversity:  0.33\n",
      "\n",
      "Document #35\n",
      "Number of sentences:  16\n",
      "Avg words per sentence:  18.88\n",
      "Unique words (vocabulary:  187\n",
      "Lexical diversity:  0.62\n",
      "\n",
      "Document #36\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  18.78\n",
      "Unique words (vocabulary:  118\n",
      "Lexical diversity:  0.7\n",
      "\n",
      "Document #37\n",
      "Number of sentences:  86\n",
      "Avg words per sentence:  26.29\n",
      "Unique words (vocabulary:  858\n",
      "Lexical diversity:  0.38\n",
      "\n",
      "Document #38\n",
      "Number of sentences:  14\n",
      "Avg words per sentence:  15.0\n",
      "Unique words (vocabulary:  137\n",
      "Lexical diversity:  0.65\n",
      "\n",
      "Document #39\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  20.67\n",
      "Unique words (vocabulary:  116\n",
      "Lexical diversity:  0.62\n",
      "\n",
      "Document #40\n",
      "Number of sentences:  10\n",
      "Avg words per sentence:  25.6\n",
      "Unique words (vocabulary:  173\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #41\n",
      "Number of sentences:  4\n",
      "Avg words per sentence:  23.75\n",
      "Unique words (vocabulary:  75\n",
      "Lexical diversity:  0.79\n",
      "\n",
      "Document #42\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  18.67\n",
      "Unique words (vocabulary:  115\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #43\n",
      "Number of sentences:  11\n",
      "Avg words per sentence:  17.45\n",
      "Unique words (vocabulary:  131\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #44\n",
      "Number of sentences:  5\n",
      "Avg words per sentence:  26.8\n",
      "Unique words (vocabulary:  95\n",
      "Lexical diversity:  0.71\n",
      "\n",
      "Document #45\n",
      "Number of sentences:  5\n",
      "Avg words per sentence:  24.4\n",
      "Unique words (vocabulary:  88\n",
      "Lexical diversity:  0.72\n",
      "\n",
      "Document #46\n",
      "Number of sentences:  41\n",
      "Avg words per sentence:  19.17\n",
      "Unique words (vocabulary:  371\n",
      "Lexical diversity:  0.47\n",
      "\n",
      "Document #47\n",
      "Number of sentences:  6\n",
      "Avg words per sentence:  13.17\n",
      "Unique words (vocabulary:  63\n",
      "Lexical diversity:  0.8\n",
      "\n",
      "Document #48\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  14.86\n",
      "Unique words (vocabulary:  76\n",
      "Lexical diversity:  0.73\n",
      "\n",
      "Document #49\n",
      "Number of sentences:  4\n",
      "Avg words per sentence:  27.0\n",
      "Unique words (vocabulary:  75\n",
      "Lexical diversity:  0.69\n",
      "\n",
      "Document #50\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  30.57\n",
      "Unique words (vocabulary:  136\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #51\n",
      "Number of sentences:  7\n",
      "Avg words per sentence:  20.14\n",
      "Unique words (vocabulary:  97\n",
      "Lexical diversity:  0.69\n",
      "\n",
      "Document #52\n",
      "Number of sentences:  64\n",
      "Avg words per sentence:  22.28\n",
      "Unique words (vocabulary:  427\n",
      "Lexical diversity:  0.3\n",
      "\n",
      "Document #53\n",
      "Number of sentences:  4\n",
      "Avg words per sentence:  14.75\n",
      "Unique words (vocabulary:  46\n",
      "Lexical diversity:  0.78\n",
      "\n",
      "Document #54\n",
      "Number of sentences:  8\n",
      "Avg words per sentence:  19.25\n",
      "Unique words (vocabulary:  99\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #55\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  21.33\n",
      "Unique words (vocabulary:  131\n",
      "Lexical diversity:  0.68\n",
      "\n",
      "Document #56\n",
      "Number of sentences:  14\n",
      "Avg words per sentence:  22.0\n",
      "Unique words (vocabulary:  179\n",
      "Lexical diversity:  0.58\n",
      "\n",
      "Document #57\n",
      "Number of sentences:  11\n",
      "Avg words per sentence:  25.09\n",
      "Unique words (vocabulary:  177\n",
      "Lexical diversity:  0.64\n",
      "\n",
      "Document #58\n",
      "Number of sentences:  9\n",
      "Avg words per sentence:  24.56\n",
      "Unique words (vocabulary:  130\n",
      "Lexical diversity:  0.59\n",
      "\n",
      "Document #59\n",
      "Number of sentences:  8\n",
      "Avg words per sentence:  18.75\n",
      "Unique words (vocabulary:  103\n",
      "Lexical diversity:  0.69\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tokenized_list)):\n",
    "  sentences = len(tokenized_list[i])\n",
    "  avg_words_sent = round(sum([len(sent) for sent in outer_list[i]]) / sentences, 2)\n",
    "  vocab = len(set([w.lower() for w in word_tokenize(big_list[i])]))\n",
    "  lex_div = round(vocab / len(word_tokenize(big_list[i])), 2)\n",
    "\n",
    "  print(f'\\nDocument #{i}')\n",
    "  print('Number of sentences: ', sentences)\n",
    "  print('Avg words per sentence: ', avg_words_sent)\n",
    "  print('Unique words (vocabulary: ', vocab)\n",
    "  print('Lexical diversity: ', lex_div)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing_Hunter_Wyatt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
