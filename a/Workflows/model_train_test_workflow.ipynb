{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split columns into:\n",
    "* `encode_cols`\n",
    "* `binary_cols`\n",
    "* `numeric_cols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"num_cols=[]\\nencode_cols=[]\\npreprocessing = ColumnTransformer(\\n    [\\n        (\\\"scale_nums\\\", StandardScaler(), num_cols),\\n        (\\\"encode_cats\\\", OneHotEncoder(drop=\\\"first\\\", sparse=False), encode_cols),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"num_cols = []\\nencode_cols = []\\npreprocessing = ColumnTransformer(\\n    [\\n        (\\\"scale_nums\\\", StandardScaler(), num_cols),\\n        (\\\"encode_cats\\\", OneHotEncoder(drop=\\\"first\\\", sparse=False), encode_cols),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = []\n",
    "encode_cols = []\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"scale_nums\", StandardScaler(), num_cols),\n",
    "        (\"encode_cats\", OneHotEncoder(drop=\"first\", sparse=False), encode_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"preprocessing\", preprocessing), (\"model name\", Model())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid= {\n",
    "    'model name + __ + hyperparam': 'some array or list',\n",
    "    . . . ,\n",
    "    . . .\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_cv= GridSearchCV(pipeline, grid, verbose=1, cv=5)\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "pipeline_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logististic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamaters\n",
    "* `penalty`\n",
    "    * l1, l2, elasticnet, none, default= l2\n",
    "* `C`\n",
    "    * default = 1.0\n",
    "* `solver`\n",
    "    * newton-cg, lbfgs, liblinear, sag, saga\n",
    "    \n",
    "Model\n",
    "* `LogisticRegressor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS Regression\n",
    "* `import statsmodels.api as sm`\n",
    "* `X = sm.add_constant(X)`\n",
    "* `results = sm.OLS(Y, X).fit()`\n",
    "\n",
    "Markov Assumptions\n",
    "* Linearity\n",
    "* Error term should be zero on average\n",
    "* Homoscedasticity\n",
    "* Low multicollinearity\n",
    "* Error terms should not be correlated\n",
    "* Features should not be correlated with errors\n",
    "* Normality of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(X, y)\n",
    "        self.predictions = self.model.predict(X)\n",
    "        self.errors = self.y - self.predictions\n",
    "        self.X_const = sm.add_constant(X)\n",
    "        self.lm_results = sm.OLS(y, self.X_const).fit()\n",
    "        self.coefs = self.model.coef_\n",
    "        self.intercepts = self.model.intercept_\n",
    "\n",
    "    def plot_linearity(self):\n",
    "        count = 1\n",
    "        plt.figure(figsize=(25, 15))\n",
    "        cols = self.X.columns\n",
    "        for col in self.X.columns:\n",
    "            plt.subplot(5, 5, count)\n",
    "            plt.scatter(self.X[col], self.predictions)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"target\")\n",
    "            count += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_homoscedasticity(self):\n",
    "        plt.scatter(self.predictions, self.errors)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residual\")\n",
    "        plt.axhline(y=0)\n",
    "        plt.title(\"Residual vs. Predicted\")\n",
    "        plt.show()\n",
    "\n",
    "    def b_pagan(self):\n",
    "        _, lmp, _, fp = het_breuschpagan(lm_results.resid, X)\n",
    "\n",
    "        return lmp, fp\n",
    "\n",
    "    def get_vifs(self):\n",
    "        vifs = []\n",
    "        for i in range(self.X_const.shape[1]):\n",
    "            vif = variance_inflation_factor(self.X_const.values, i)\n",
    "            vifs.append(vif)\n",
    "\n",
    "        return pd.Series(vifs, index=self.X_const.columns)\n",
    "\n",
    "    def plot_errors(self):\n",
    "        plt.plot(self.errors)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_errors_acf(self):\n",
    "        acf_data = acf(self.errors)\n",
    "\n",
    "        plt.plot(acf_data[1:])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_error_normality(self):\n",
    "        qqplot(lm_results.resid, line=\"s\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.hist(lm_results.resid)\n",
    "        plt.show()\n",
    "\n",
    "    def shapiro_wilkes(self):\n",
    "        return stats.shapiro(self.lm_results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "* the process of modifying algorithms in order to lower the generalization gap without sacrificing training performance\n",
    "* `ridgeregr = Ridge(alpha=10**37)`\n",
    "    * Ridge regression shrinks parameter estimates, but the estimates never reach exactly 0\n",
    "* `lasso_cv = LassoCV(alphas=alphas, cv=5)`\n",
    "    * Works by trying to force small parameter estimates to be equal to zero, effectively dropping them from the model\n",
    "* `elasticregr = ElasticNet(alpha=10**21, l1_ratio=0.5)`\n",
    "    * Combination of Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamaters\n",
    "* `max_depth`\n",
    "    * too high--> overfit\n",
    "* `max_leaf_nodes`\n",
    "    * too high--> overfit\n",
    "* `min_samples_leaf`\n",
    "    * too low--> overfit\n",
    "* `n_estimators`\n",
    "    * too high--> overfit\n",
    "* `max_features\n",
    "    * too high--> overfit\n",
    "* `max_samples`\n",
    "    * too high--> overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "importances = model.best_estimator_.feature_importances_\n",
    "importances\n",
    "importance_df = pd.DataFrame({\"feat\": X_train.columns, \"importance\": importances})\n",
    "importance_df = importance_df.sort_values(\"importance\", ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entropy:\n",
    "    ''' \n",
    "    params: \n",
    "        df: Categorical dataframe with no nulls\n",
    "        target: The column name of the target variable of the model\n",
    "        \n",
    "    get_wae_d: Returns a dict: {feature: {value: weighted average entropy}}\n",
    "    \n",
    "    display_wae: params: Feature to display\n",
    "                 function: displays weighted average \n",
    "                           entropy of each value of each feature\n",
    "                         \n",
    "    display_best_questions: Display the best question (smallest \n",
    "                            weighted average entropy) of each feature\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, target):\n",
    "        # Initialize class\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.columns = df.drop(columns=[target]).columns\n",
    "        self.col_values_d = self.get_col_values_d\n",
    "        self.wae_d=self.get_wae_d\n",
    "        self.best_questions= {k: min(v,key=v.get) for k,v in self.wae_d().items()}\n",
    "\n",
    "    def get_col_values_d(self):\n",
    "        # Returns {feature: [value,value,...], ...}\n",
    "        col_values_d = dict()\n",
    "        for col in self.columns:\n",
    "            col_values_d[col] = tennis[col].unique()\n",
    "\n",
    "        return col_values_d\n",
    "\n",
    "    def get_entropy(self,probs):\n",
    "        # Returns entropy of given probabilities\n",
    "        entropy = 0\n",
    "        for prob in probs:\n",
    "            entropy += -prob * np.log2(prob)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def get_wae_d(self):\n",
    "        # Returns {feature: {value: weighted average entropy}, ...}\n",
    "        entropy_d = dict()\n",
    "        \n",
    "        # Iterate through values of each feature\n",
    "        # Fill a dict of dicts with features, values, and entropy\n",
    "        for k, v in self.col_values_d().items(): \n",
    "            value_dict = dict()\n",
    "            for value in v:\n",
    "                yes = self.df[self.df[k] == value]\n",
    "                no= self.df[self.df[k] != value]\n",
    "                yes_probs = yes[self.target].value_counts(normalize=True)\n",
    "                no_probs = no[self.target].value_counts(normalize=True)\n",
    "                yes_entropy = self.get_entropy(yes_probs)\n",
    "                no_entropy= self.get_entropy(no_probs)\n",
    "                no_weight= no.shape[0]/ self.df.shape[0]\n",
    "                yes_weight= yes.shape[0]/ self.df.shape[0]\n",
    "                weighted_average_entropy= yes_weight*yes_entropy+no_weight*no_entropy\n",
    "                value_dict[value] = weighted_average_entropy\n",
    "            entropy_d[k] = value_dict\n",
    "        \n",
    "        return entropy_d\n",
    "    \n",
    "    def display_wae(self,column):\n",
    "        # Display weighted average entropy\n",
    "        for k,v in self.wae_d().items():\n",
    "            if k==column:\n",
    "                print(k,'\\n')\n",
    "                for key,value in v.items():\n",
    "                    print(key+':',round(value,5),'\\n')\n",
    "                    \n",
    "\n",
    "    def display_best_questions(self):\n",
    "        # Display values with smallest weighted average entropy\n",
    "        for k,v in self.best_questions.items():\n",
    "            print(k,':',v,'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
