{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKCze2_9L1NZ"
   },
   "source": [
    "### Factor Analysis Assignment Solution\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "Objectives:    \n",
    "- **Students can explain factor analysis**\n",
    "\n",
    "- **Students understand how factor analysis differs from PCA and ICA**\n",
    "\n",
    "- **Students can identify common use cases for factor analysis**\n",
    "\n",
    "- Students can apply factor analysis to an appropriate dataset (i.e. purely quantitative)\n",
    "\n",
    "- Students can explain dimensionality reduction techniques for categorical (MCA) and mixed (FAMD) data\n",
    "\n",
    "- Students can apply MCA and FAMD to appropriate datasets\n",
    "\n",
    "References:\n",
    "\n",
    "-  Dr J Maiti, IIT KharagpurMod-01, Lec-33 Factor Analysis    \n",
    "https://www.youtube.com/watch?v=n3y3xLNoPk4\n",
    "\n",
    "- Barbara Engelhardt, Factor Analysis   \n",
    "https://www.cs.princeton.edu/~bee/courses/scribe/lec_10_02_2013.pdf\n",
    "\n",
    "- Wikipedia - Factor Analysis    \n",
    "https://en.wikipedia.org/wiki/Factor_analysis   \n",
    "\n",
    "- Factor Analysis - an introduction     \n",
    "https://www.youtube.com/watch?v=WV_jcaDBZ2I\n",
    "\n",
    "- Introduction to Factor Analysis in Python   \n",
    "https://www.datacamp.com/community/tutorials/introduction-factor-analysis\n",
    "\n",
    "- Factor Analysis in Python  \n",
    "https://www.youtube.com/watch?v=ttBs_wfw_6U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5R-LwYjL1Nd"
   },
   "source": [
    "### Factor Analysis\n",
    "\n",
    "Factor Analysis is a method for modeling observed variables. It is commonly used for data analysis and dimensionality reduction. The key concept of Factor Analysis is that some of variables have similar patterns because they are all related to factors (latent or unobserved variables) that cannot be directly measured. The factors typically are viewed as broad concepts or ideas that may describe an observed phenomenon.\n",
    "\n",
    "For example,\n",
    "\n",
    "- A psychologist is interested to measure a person's mental ability\n",
    "\n",
    "- A cardiologist wants to measure risk of a heart attack   \n",
    "\n",
    "- A marketing manager is interested to measure the purchase intention of customers\n",
    "\n",
    "- A psychiatrist wants to measure depression   \n",
    "\n",
    "- A supply chain analyst is interested in measuring supply chain coordination .  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WlnJo0QL1Ne"
   },
   "source": [
    "### Applying factor analysis\n",
    "\n",
    "- Find the hidden factors behind observed variables: The hidden factors cannot measured directly, but can be interpretable.\n",
    "\n",
    "- Estimate the factor loadings.\n",
    "\n",
    "- Evaluate the factors we extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0j6zIZ7L1Ng"
   },
   "source": [
    "#### Data\n",
    "\n",
    "The dataset we work on is called `bfi` from Psych R-package. It consists of 2800 observations on 28 variables including 25 self-report personality items and three additional demographic variables (sex, education, and age). In this project, we plan to figure out the  factors of the first 25 items in dataset.\n",
    "\n",
    "Dataset: https://rdrr.io/cran/psych/man/bfi.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyIOmtI7L1Nh"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYAOAwlpL1Nm"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'factor_analyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1718af98f149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfactor_analyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFactorAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'factor_analyzer'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKcsMapDL1Np"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/bfi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7QdiqJhL1Ns"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKgpFtWeL1Nv"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFOp0NN5L1Ny"
   },
   "outputs": [],
   "source": [
    "df.drop(['gender', 'education', 'age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81brx0vWL1N1"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G46LJDSwL1N3"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aLj1_aqL1N7"
   },
   "outputs": [],
   "source": [
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpQVJtInL1N9"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l2So0uHL1OA"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSHfy0ShL1OD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(df.corr())\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLtw7QKCL1OH"
   },
   "source": [
    "### Perform Factor Analysis\n",
    "\n",
    "factor_analyzer package documentations:   \n",
    "https://factor-analyzer.readthedocs.io/en/latest/factor_analyzer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "ZLNEDcvCL1OH"
   },
   "source": [
    "# Ensure that the factor_analyzer package is installed \n",
    "pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE0Xec_aL1OI"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PS8Byk5DL1OL"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/bfi.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hn_96IIPL1OO"
   },
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "df.drop(['gender', 'education', 'age'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOvb3jgwL1OQ"
   },
   "outputs": [],
   "source": [
    "# Dropping missing values rows\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUIbnw-nL1OS"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4icBIxqdL1OV"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRwcWdcxL1OX"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oitt3dMvL1Og"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLunwqNfL1Om"
   },
   "source": [
    "#### Adequacy Test\n",
    "\n",
    "Before you perform factor analysis, best practice is to evaluate the “factorability” of our dataset. Factorability means \"can we found the factors in the dataset?\". There are two methods to check the factorability or sampling adequacy:\n",
    "\n",
    "- Bartlett’s Test\n",
    "\n",
    "- Kaiser-Meyer-Olkin Test\n",
    "\n",
    "Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EK-L6tRL1Oo"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGcwgGZPL1Oq"
   },
   "source": [
    "Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvgbbQtmL1Or"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ab0b9OjqL1Ot"
   },
   "outputs": [],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tZVCoE0L1Ow"
   },
   "source": [
    "The overall KMO for our data is 0.84, which is excellent. This value indicates that you can proceed with your planned factor analysis.\n",
    "\n",
    "#### Choosing the Number of Factors\n",
    "\n",
    "For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVXNa-uAL1Ox"
   },
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(df, 25)\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BR5euD8L1Oz"
   },
   "source": [
    "Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvqBnjdKL1O0"
   },
   "outputs": [],
   "source": [
    "# Create scree plot using matplotlib\n",
    "plt.scatter(range(1,df.shape[1]+1),ev)\n",
    "plt.plot(range(1,df.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgKZWFb9L1O2"
   },
   "source": [
    "### Things to consider    \n",
    "\n",
    "Factor analysis is typically done using the “varimax” rotation method. What if we utilize other kinds of rotation, such as “quartimax” , “bentlerT”, “equamax” and etc. Do they change the results dramatically or not?\n",
    "\n",
    "In this case, the type of factor analysis we use is exploratory factor anaylsis (EFA). There is another type of factor analysis, `Confirmatory factor analysis (CFA)` which is a more complex approach that tests the hypothesis that the items are associated with specific factors. What if we do the CFA on the example dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFkw4HDaL1O2"
   },
   "source": [
    "### Assignment: FCA on Kaggle Big Mart Sales Dataset\n",
    "\n",
    "The big-mart-sales-dataset contains images of products contains 60,000 28x28 3-channel images. Originally, the goal was to predict sales from images. We are using the images for factor analysis.   \n",
    "https://www.kaggle.com/aakash2016/big-mart-sales-dataset  \n",
    "\n",
    "Step 6 is your primary task in this assignment.\n",
    "\n",
    "1) Download the big-mart-sales-dataset \n",
    "\n",
    "2) Install opencv: conda install -c menpo opencv\n",
    "\n",
    "3) Install the factor_analyzer package: https://factor-analyzer.readthedocs.io/en/latest/factor_analyzer.html\n",
    "\n",
    "4) Create a correlation matrix, apply the KMO adequacy test, and generate a scree plot.\n",
    "\n",
    "5) From the KMO/scree plot identify the optimal number of factors\n",
    "\n",
    "6) Apply factor analysis with your identified optimal number of factors.\n",
    "\n",
    "7) Compare ICA reconstructed images with PCA.\n",
    "\n",
    "The notebook, DimensionalityReduction_52_1_fa_assign.ipynb, has a complete example with an arbitrary (very small) number of factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDrCdHa_L1O3"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the training file to get labels\n",
    "f = 'https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Train_UWu5bXk.csv'\n",
    "# read the data\n",
    "train=pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1575996538551,
     "user": {
      "displayName": "Mike Swirsky",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEvj_9k9NYcdWSI5G-tvxoZDUz8RATiP7-zeMD=s64",
      "userId": "09733430627481200667"
     },
     "user_tz": 480
    },
    "id": "yMy6coF_L1O5",
    "outputId": "b9e99ebe-25bb-4592-c701-df86ad76ffb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier               0.000000\n",
       "Item_Weight                  17.165317\n",
       "Item_Fat_Content              0.000000\n",
       "Item_Visibility               0.000000\n",
       "Item_Type                     0.000000\n",
       "Item_MRP                      0.000000\n",
       "Outlet_Identifier             0.000000\n",
       "Outlet_Establishment_Year     0.000000\n",
       "Outlet_Size                  28.276428\n",
       "Outlet_Location_Type          0.000000\n",
       "Outlet_Type                   0.000000\n",
       "Item_Outlet_Sales             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the percentage of missing values in each variable\n",
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1178,
     "status": "ok",
     "timestamp": 1575996581500,
     "user": {
      "displayName": "Mike Swirsky",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEvj_9k9NYcdWSI5G-tvxoZDUz8RATiP7-zeMD=s64",
      "userId": "09733430627481200667"
     },
     "user_tz": 480
    },
    "id": "Ar32LflbL1O9",
    "outputId": "de66292b-d45b-4301-f8f7-90df6bc2cdc5",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight  ...        Outlet_Type  Item_Outlet_Sales\n",
       "0           FDA15         9.30  ...  Supermarket Type1          3735.1380\n",
       "1           DRC01         5.92  ...  Supermarket Type2           443.4228\n",
       "2           FDN15        17.50  ...  Supermarket Type1          2097.2700\n",
       "3           FDX07        19.20  ...      Grocery Store           732.3800\n",
       "4           NCD19         8.93  ...  Supermarket Type1           994.7052\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmII5yVLwATb"
   },
   "source": [
    "Copy the zip file onto your Google Drive. Unzip the file, then put the 60,000 images into a numpy array to prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2HnRmCYgPZV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXZl1IErL1PG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if os.path.exists('/content/gdrive/My Drive/data/train_LbELtWX'):\n",
    "    pass\n",
    "else:\n",
    "    with ZipFile('/content/gdrive/My Drive/train_LbELtWX.zip', 'r') as zipObj:\n",
    "        # Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall('/content/gdrive/My Drive/data/train_LbELtWX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rh4dE6wzL1PJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "images = [cv2.imread(file) for file in glob('/content/gdrive/My Drive/data/train_LbELtWX/train/*.png')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMQPj9h5L1PL"
   },
   "source": [
    "Now we will convert these images into a numpy array format so that we can perform mathematical operations on them and also plot the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1575996504333,
     "user": {
      "displayName": "Mike Swirsky",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEvj_9k9NYcdWSI5G-tvxoZDUz8RATiP7-zeMD=s64",
      "userId": "09733430627481200667"
     },
     "user_tz": 480
    },
    "id": "new8WqkNL1PM",
    "outputId": "d2b6731e-7452-4548-b593-363995d5fa3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmrSRacoL1PV"
   },
   "source": [
    "As you can see above, it’s a 3-dimensional array. We must convert it to 1-dimension as all the upcoming techniques only take 1-dimensional input. To do this, we need to flatten the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_e5l0DyL1PW"
   },
   "outputs": [],
   "source": [
    "image = []\n",
    "for i in range(0,60000):\n",
    "    img = images[i].flatten()\n",
    "    image.append(img)\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hhmzalj5L1PY"
   },
   "source": [
    "Let us now create a dataframe containing the pixel values of every individual pixel present in each image, and also their corresponding labels (for labels, we will make use of the train.csv file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mnugY7RL1PZ"
   },
   "source": [
    "As you can see in the above table, there aren’t too many missing values (just 2 variables have them actually). We can impute the values using appropriate methods, or we can set a threshold of, say 20%, and remove the variable having more than 20% missing values. Let’s look at how this can be done in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8NlijqWL1Pb"
   },
   "outputs": [],
   "source": [
    "feat_cols = [ 'pixel'+str(i) for i in range(image.shape[1]) ]\n",
    "df = pd.DataFrame(image,columns=feat_cols)\n",
    "# df['label'] = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dal1IxD5L1Pe"
   },
   "source": [
    "#### Now you will decompose the dataset using Factor Analysis:\n",
    "\n",
    "1) Create a FactorAnalysis object for `n_components`\n",
    "\n",
    "2) Use your FA object to `fit_transform` your `feat_cols` in dataframe `df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZC40RoXwL1Pf"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "FA = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcgZE-SsL1Ph"
   },
   "source": [
    "Here, `n_components` will decide the number of factors in the transformed data. \n",
    "\n",
    "After transforming the data, it’s time to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-xaKdMUL1Ph"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Factor Analysis Components')\n",
    "plt.scatter(FA[:,0], FA[:,1])\n",
    "plt.scatter(FA[:,1], FA[:,2])\n",
    "plt.scatter(FA[:,2],FA[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFX5_Oy9L1Pj"
   },
   "source": [
    "Interesting. We can see all the different factors in the above graph. Here, the x-axis and y-axis represent the values of decomposed factors. \n",
    "\n",
    "It is hard to observe these factors individually, but we have been able to reduce the dimensions of our data successfully.\n",
    "\n",
    "#### Plot the images using the independent components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AH19K-XoL1Pk"
   },
   "outputs": [],
   "source": [
    "rndperm = np.random.permutation(df.shape[0])\n",
    "plt.gray()\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "for i in range(0,15):\n",
    "    ax = fig.add_subplot(3,5,i+1)\n",
    "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28*3)).astype(float))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVLKTjTXL1Pq"
   },
   "source": [
    "#### Lets use PCA and compare the reconstructed images with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wiwfVCpL1Pr"
   },
   "outputs": [],
   "source": [
    "#Let’s implement PCA using Python and transform the dataset:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7ceTqrvL1Px"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(4), pca.explained_variance_ratio_, color=\"red\")\n",
    "plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_), color=\"blue\")\n",
    "plt.title(\"Component-wise and Cumulative Explained Variance\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPxN_18vL1Pz"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(12, 8))\n",
    "sns.heatmap(pca.components_[0, :].reshape(28, 84), ax=axarr[0][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[1, :].reshape(28, 84), ax=axarr[0][1], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[2, :].reshape(28, 84), ax=axarr[1][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[3, :].reshape(28, 84), ax=axarr[1][1], cmap='gray_r')\n",
    "axarr[0][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[0]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[1]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[2]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[3]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][0].set_aspect('equal')\n",
    "axarr[0][1].set_aspect('equal')\n",
    "axarr[1][0].set_aspect('equal')\n",
    "axarr[1][1].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('4-Component PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPNS8jhhL1P1"
   },
   "source": [
    "Each additional dimension we add to the PCA technique captures less and less of the variance in the model. The first component is the most important one, followed by the second, then the third, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cC7dEGNBL1P1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DimensionalityReduction_52_1_fca_assign.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
